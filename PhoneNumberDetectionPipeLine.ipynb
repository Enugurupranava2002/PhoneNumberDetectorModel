{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from data_load import PhoneNumberDataset\n",
    "from data_load import Normalize, Gray_scale_Image, ToTensor, Gray_scale_to_Binary, BoundingBoxes\n",
    "\n",
    "data_transform = transforms.Compose([Gray_scale_Image(), BoundingBoxes(), ToTensor()])\n",
    "\n",
    "assert(data_transform is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_valid_df = pd.read_csv(\"data\\cross_valid_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>phone_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phoneNumberImage22542.png</td>\n",
       "      <td>2599025368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phoneNumberImage17984.png</td>\n",
       "      <td>3212222661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phoneNumberImage1575.png</td>\n",
       "      <td>8523749904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phoneNumberImage30823.png</td>\n",
       "      <td>1931705751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phoneNumberImage39894.png</td>\n",
       "      <td>350417540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image_id  phone_number\n",
       "0  phoneNumberImage22542.png    2599025368\n",
       "1  phoneNumberImage17984.png    3212222661\n",
       "2   phoneNumberImage1575.png    8523749904\n",
       "3  phoneNumberImage30823.png    1931705751\n",
       "4  phoneNumberImage39894.png     350417540"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_valid_dataset = PhoneNumberDataset(\"data\\cross_valid_df.csv\", \"data\\phone_number_images\", transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "cross_valid_dataloader = DataLoader(cross_valid_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (maxPool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (mpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1600, out_features=80, bias=True)\n",
       "  (fc1_drop): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=80, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.load(\"data\\models\\model_basic\\modelWith2EpochOperaAppliedOnEachDigitBatchSize320.pth\")\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_sample_output():\n",
    "    for i, sample in enumerate(cross_valid_dataloader):\n",
    "        images = sample['image']\n",
    "        phone_num_digits_list = sample['digits_list']\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        output_digits = net(images)\n",
    "        print(output_digits[i][1])\n",
    "        output_digits = torch.argmax(output_digits, dim=1)\n",
    "        output_digits = output_digits.view(output_digits.size()[0], -1)\n",
    "        \n",
    "        if i == 0:\n",
    "            return images, output_digits, phone_num_digits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<SelectBackward0>)\n",
      "torch.Size([10, 10, 28, 28])\n",
      "torch.Size([100, 1])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def show_Image(image):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "\n",
    "test_images, test_outputs, ground_truth_output = net_sample_output()\n",
    "print(test_images.data.size())\n",
    "print(test_outputs.data.size())\n",
    "print(len(ground_truth_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_Image(image):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "\n",
    "def visualize_output(test_images, test_outputs, ground_truth_output, batch_size=4):\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        image = test_images[i].data\n",
    "        image = image.numpy()\n",
    "\n",
    "        test_digits = test_outputs[i*10:i*10 + 10].data.reshape((10))\n",
    "        test_digits = test_digits.numpy()\n",
    "        \n",
    "        print(\"test digits:   \", test_digits)\n",
    "        \n",
    "        grond_digits = [x.data.numpy() for x in ground_truth_output]\n",
    "        grond_digits = np.asarray(grond_digits).T\n",
    "        grond_digits = grond_digits[i]\n",
    "\n",
    "        print(\"ground digits: \", grond_digits)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        digits_list = [x.data.numpy() for x in ground_truth_output]\n",
    "        digits_batch = torch.Tensor(np.transpose(np.asarray(digits_list)))\n",
    "        digits_one_hot = []\n",
    "        for phone_num in digits_batch:\n",
    "            one_hot_encoded_digit = F.one_hot(phone_num.type(torch.LongTensor), num_classes=10)\n",
    "            digits_one_hot.append(one_hot_encoded_digit)\n",
    "        digits_one_hot_tensor = [x.data.numpy() for x in digits_one_hot]\n",
    "        digits_one_hot_tensor = np.array(digits_one_hot_tensor)\n",
    "        digits_one_hot_tensor = torch.Tensor(digits_one_hot_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test digits:    [3 1 7 4 9 6 6 1 1 9]\n",
      "ground digits:  [3 1 7 4 9 6 6 1 1 8]\n",
      "\n",
      "\n",
      "test digits:    [8 8 0 2 0 3 8 6 4 8]\n",
      "ground digits:  [8 8 0 2 0 3 8 6 4 8]\n",
      "\n",
      "\n",
      "test digits:    [1 8 9 3 4 2 3 2 1 1]\n",
      "ground digits:  [1 8 9 3 4 2 3 2 1 1]\n",
      "\n",
      "\n",
      "test digits:    [4 9 7 8 6 5 8 7 4 7]\n",
      "ground digits:  [4 9 2 8 6 5 8 7 4 7]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_output(test_images, test_outputs, ground_truth_output=ground_truth_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on Cross validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_net():\n",
    "    \n",
    "    net.eval()\n",
    "    running_accu = 0.0\n",
    "\n",
    "    for batch_i, data in enumerate(cross_valid_dataloader):\n",
    "\n",
    "        images = data['image']\n",
    "        digits_list = data['digits_list']\n",
    "\n",
    "        digits_list = [x.data.numpy() for x in digits_list]\n",
    "        digits_batch = torch.Tensor(np.transpose(np.asarray(digits_list)))\n",
    "        # print(digits_batch)\n",
    "        digits_batch = torch.reshape(digits_batch,(digits_batch.size(0)*digits_batch.size(1), ))\n",
    "        # print(digits_batch.size())\n",
    "        digits_one_hot = []\n",
    "        for phone_num in digits_batch:\n",
    "            one_hot_encoded_digit = F.one_hot(phone_num.type(torch.LongTensor), num_classes=10)\n",
    "            digits_one_hot.append(one_hot_encoded_digit)\n",
    "        digits_one_hot_tensor = [x.data.numpy() for x in digits_one_hot]\n",
    "        digits_one_hot_tensor = np.array(digits_one_hot_tensor)\n",
    "        digits_one_hot_tensor = torch.Tensor(digits_one_hot_tensor)\n",
    "        images = images.type(torch.FloatTensor)\n",
    "\n",
    "        output_digits = net(images)\n",
    "        \n",
    "        output_digits = output_digits.type(torch.FloatTensor)\n",
    "        #Accuracy\n",
    "        out = torch.argmax(output_digits, dim=1)            \n",
    "        # print(out.size())\n",
    "        accuracy = out == digits_batch\n",
    "        accuracy = torch.sum(accuracy.detach()).float()\n",
    "\n",
    "        running_accu += accuracy\n",
    "\n",
    "        if(batch_i % 10 == 9):\n",
    "            print(f'Batch: {batch_i + 1}, Avg. Accu: {running_accu}')\n",
    "            running_accu = 0.0\n",
    "            \n",
    "    print(\"Cross Validation is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 10, Avg. Accu: 966.0\n",
      "Batch: 20, Avg. Accu: 955.0\n",
      "Batch: 30, Avg. Accu: 952.0\n",
      "Batch: 40, Avg. Accu: 956.0\n",
      "Batch: 50, Avg. Accu: 965.0\n",
      "Batch: 60, Avg. Accu: 962.0\n",
      "Batch: 70, Avg. Accu: 956.0\n",
      "Batch: 80, Avg. Accu: 946.0\n",
      "Batch: 90, Avg. Accu: 944.0\n",
      "Batch: 100, Avg. Accu: 947.0\n",
      "Batch: 110, Avg. Accu: 957.0\n",
      "Batch: 120, Avg. Accu: 962.0\n",
      "Batch: 130, Avg. Accu: 966.0\n",
      "Batch: 140, Avg. Accu: 938.0\n",
      "Batch: 150, Avg. Accu: 950.0\n",
      "Batch: 160, Avg. Accu: 951.0\n",
      "Batch: 170, Avg. Accu: 959.0\n",
      "Batch: 180, Avg. Accu: 960.0\n",
      "Batch: 190, Avg. Accu: 965.0\n",
      "Batch: 200, Avg. Accu: 953.0\n",
      "Batch: 210, Avg. Accu: 945.0\n",
      "Batch: 220, Avg. Accu: 953.0\n",
      "Batch: 230, Avg. Accu: 959.0\n",
      "Batch: 240, Avg. Accu: 954.0\n",
      "Batch: 250, Avg. Accu: 954.0\n",
      "Batch: 260, Avg. Accu: 962.0\n",
      "Batch: 270, Avg. Accu: 950.0\n",
      "Batch: 280, Avg. Accu: 948.0\n",
      "Batch: 290, Avg. Accu: 949.0\n",
      "Batch: 300, Avg. Accu: 961.0\n",
      "Batch: 310, Avg. Accu: 957.0\n",
      "Batch: 320, Avg. Accu: 956.0\n",
      "Batch: 330, Avg. Accu: 962.0\n",
      "Batch: 340, Avg. Accu: 957.0\n",
      "Batch: 350, Avg. Accu: 951.0\n",
      "Batch: 360, Avg. Accu: 957.0\n",
      "Batch: 370, Avg. Accu: 963.0\n",
      "Batch: 380, Avg. Accu: 962.0\n",
      "Batch: 390, Avg. Accu: 969.0\n",
      "Batch: 400, Avg. Accu: 961.0\n",
      "Batch: 410, Avg. Accu: 954.0\n",
      "Batch: 420, Avg. Accu: 953.0\n",
      "Batch: 430, Avg. Accu: 964.0\n",
      "Batch: 440, Avg. Accu: 973.0\n",
      "Batch: 450, Avg. Accu: 957.0\n",
      "Batch: 460, Avg. Accu: 964.0\n",
      "Batch: 470, Avg. Accu: 949.0\n",
      "Batch: 480, Avg. Accu: 971.0\n",
      "Batch: 490, Avg. Accu: 952.0\n",
      "Batch: 500, Avg. Accu: 961.0\n",
      "Cross Validation is finished\n"
     ]
    }
   ],
   "source": [
    "cross_valid_net()# total images are 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bc6de047e58dac5738e6fd71455bb891a228ea6d3201e9c8a63259fc0a2df17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
